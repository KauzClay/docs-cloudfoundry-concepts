---
title: Understanding GrootFS Disk Usage
owner: Security
---

<strong><%= modified_date %></strong>

This topic describes how GrootFS manages disk space in <%= vars.product_full %>.

## <a id='how-much'></a>How much disk space is GrootFS using?

To reconcile disk allocations for containers with the actual disk usage on the host VM, you need to understand how GrootFS uses its disk.
Command line tools such as `du` and `df` can provide misleading information because of the way container file systems work.

For example:

- The Diego cell rep appears to be out of disk capacity to allocate, but the actual disk usage on the Garden host is relatively low. This results in containers not being placed on the cell.
- The Diego cell rep appears to have disk space to allocate, but the space used by containers and system components together prevents Diego from allocating the remaining disk space. This results in containers continuing to be placed on the cell and then failing to start.

These commands are designed to be run on a BOSH-deployed VM running the Garden job.

## <a id='exclusive-disk'></a>How much disk is used exclusive to container X?
On disk, the read-write layer for each container can be found at `/var/vcap/data/grootfs/store/unprivileged/images/CONTAINER-ID/diff`.
When GrootFS calls on the built-in XFS quota tooling to get disk usage for a container, it takes into account data written to that directory and not the data in the read-only volumes.

Running `grootfs stats` returns the following values:

* `total_bytes_used`: This is the disk usage of the container **including** the rootfs image volumes.
* `exclusive_bytes_used`: This is the disk usage of the container **not** including the rootfs image volumes.

Run the following command to look up the container ID:
<pre class="terminal">
$ ls /var/vcap/data/garden/depot/
</pre>
The command above returns output in the following format:
<pre class="terminal">
55afbf65-5cbf-49c6-4461-f803
</pre>

To check a container's disk usage, run the following command, replacing `CONTAINER-ID` with the container ID from the output of the previous command:

```
/var/vcap/packages/grootfs/bin/grootfs --config \
/var/vcap/jobs/garden/config/grootfs_config.yml stats CONTAINER-ID
```

For example:
<pre class="terminal">
$ /var/vcap/packages/grootfs/bin/grootfs --config \
/var/vcap/jobs/garden/config/grootfs\_config.yml stats 55afbf65-5cbf-49c6-4461-f803
</pre>
<p class="note"><strong>Note</strong>: For privileged containers, replace <code>grootfs\_config.yml</code> in the above command with <code>privileged\_grootfs\_config.yml</code>.</p>
The command above returns output in the following format:
<pre class="terminal">
{"disk\_usage":{"total\_bytes\_used":23448093,"exclusive\_bytes\_used":8192}}
</pre>


## How much exclusive disk is used across all running containers?
```sh
$ ls /var/vcap/data/garden/depot/ \
    | xargs -I{} /var/vcap/packages/grootfs/bin/grootfs --config \
    /var/vcap/jobs/garden/config/grootfs_config.yml stats {}  \
    | cut -d: -f4 | cut -d} -f1 | awk '{sum += $1} END {print sum}'
# returns total in bytes
```
If you believe you are also creating privileged containers, update the config path in the above command to use `privileged_grootfs_config.yml`.

## How much disk does underlying layer Y (rootfs or, in future, droplet or other non-root layer) use?
Underlying layers are known as `volumes` in GrootFS. They are read-only and their changesets are layered together through an `overlay` mount to create the rootfs for containers.
When GrootFS writes each filesystem layer/volume to disk, it also stores the number of bytes written to a file in `meta`. To find out the size of an individual layer/volume, we just need to
read the corresponding metadata file.

```sh
$ cat /var/vcap/data/grootfs/store/unprivileged/meta/volume-<abcd-layer-sha>
{"Size":5607885} # unit in bytes
```
We can also use `du` (passing the absolute path to the volume) to find out the same thing.
```sh
$ du -sch /var/vcap/data/grootfs/store/unprivileged/volumes/<abcd-layer-sha>/
5.4M    /var/vcap/data/grootfs/store/unprivileged/volumes/<abcd-layer-sha>/
```
If you believe you are also creating privileged containers, update the store paths in the above command.

## How much disk do all the active layers use in total?
For each container, GrootFS mounts the underlying volumes using `overlay` to a point in the `images` directory. This point is the rootfs for the container and is read-write.
GrootFS also stores the shas of each underlying volume used by an image in the `meta` folder. To find out which layers/volumes are active, we first parse all the dependency metadata
for every image (container rootfs), remove the duplicates, and then read each volume's metadata file (as we did in the section above). Finally we can sum the bytes of all active volumes on disk.

```sh
$ for image in $(ls /var/vcap/data/grootfs/store/unprivileged/meta/dependencies/image\:*.json); \
    do cat $image | python -c 'import json,sys;obj=json.load(sys.stdin); \
    print "\n".join(obj)' ; done | sort -u | xargs -I{} cat /var/vcap/data/grootfs/store/unprivileged/meta/volume-{} | cut -d : -f 2 | cut -d} -f1 \
    | awk '{sum += $1} END {print sum}'
# returns total in bytes
```
If you believe you are also creating privileged containers, update the store paths in the above command.

## How much disk does the store use in total?
```sh
$ df | grep -E  "/var/vcap/data/grootfs/store/(privileged|unprivileged)$" | awk '{sum += $3} END {print sum}'
# returns total in bytes
```

## How much disk could be reclaimed through pruning unused layers?
You can use values gathered from commands above to calculate how much space could be reclaimed by pruning unused layers: total store disk usage - active layers.

## Are there categories of grootfs disk usage that should be accounted for, other than the ones above? If so, how much disk usage do they account for?
The bulk of disk usage will go to the `images/<id>/diff` and `volumes` directories (found under `/var/vcap/data/grootfs/store/{unprivileged,privileged}/`).


GrootFS also stores information in the following directories:
 - `l` -> link dirs. Shorter dir-names symlinked to volume dirs to allow groot to union mount more layers/filepaths.
 - `locks` -> filesystem lock dir to ensure safety during concurrent cleans/creates.
 - `meta` -> per image and volume metadata.
 - `projectids` -> empty numbered directories used to track image quotas.
 - `tmp` -> normal tempdir stuff.


These would would usually total less than 2MB.

## If I force grootfs to prune its cache, would I be able to reduce total disk usage to Z% of local disk or to W GB?
It would rarely be necessary to force GrootFS to clean its cache, but if more space is needed on disk, then operators should set the `reserved_space_for_other_jobs_in_mb` property to a higher value.
How much space is freed would depend on how many layers/volumes are actively used by images/rootfses. You can use values gathered from commands above to calculate the space which
could be cleared by a force clean: total disk in use by store - total disk used by active layers.

## Is grootfs using more disk than it is configured to use, or is it behaving correctly?
Grootfs stores are [initialised](https://github.com/cloudfoundry/garden-runc-release/blob/b4a44c5cabb1570eaeb25b158823cfbd97ae530c/jobs/garden/templates/bin/overlay-xfs-setup#L23-L46) to use the [entire](https://github.com/cloudfoundry/garden-runc-release/blob/b4a44c5cabb1570eaeb25b158823cfbd97ae530c/jobs/garden/templates/bin/grootfs-utils.erb#L31) `/var/vcap/data`. If the `reserved_space_for_other_jobs_in_mb` is not set high enough, or if there are many images with few shared volumes, it could use it all.
The [Thresholder](https://github.com/cloudfoundry/garden-runc-release/blob/b4a44c5cabb1570eaeb25b158823cfbd97ae530c/src/thresholder/main.go) calculates and sets a value so that Grootâ€™s GC will try to ensure that a small reserved space is kept free for other jobs. Groot will only try to GC when that threshold is reached.
However, if all the rootfs layers are active (in use by images), then GC cannot occur and that space will be used.

